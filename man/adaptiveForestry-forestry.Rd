% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/adaptive_forestry.R
\name{adaptiveForestry-forestry}
\alias{adaptiveForestry-forestry}
\alias{adaptiveForestry}
\title{forestry with adaptive featureWeights}
\usage{
adaptiveForestry(
  x,
  y,
  ntree = 500,
  ntree.first = 25,
  ntree.second = 500,
  replace = TRUE,
  sampsize = if (replace) nrow(x) else ceiling(0.632 * nrow(x)),
  sample.fraction = NULL,
  mtry = max(floor(ncol(x)/3), 1),
  nodesizeSpl = 5,
  nodesizeAvg = 5,
  nodesizeStrictSpl = 1,
  nodesizeStrictAvg = 1,
  minSplitGain = 0,
  maxDepth = round(nrow(x)/2) + 1,
  interactionDepth = maxDepth,
  interactionVariables = numeric(0),
  featureWeights = NULL,
  deepFeatureWeights = NULL,
  observationWeights = NULL,
  splitratio = 1,
  OOBhonest = FALSE,
  seed = as.integer(runif(1) * 1000),
  verbose = FALSE,
  nthread = 0,
  splitrule = "variance",
  middleSplit = FALSE,
  maxObs = length(y),
  linear = FALSE,
  linFeats = 0:(ncol(x) - 1),
  monotonicConstraints = rep(0, ncol(x)),
  monotoneAvg = FALSE,
  overfitPenalty = 1,
  doubleTree = FALSE,
  reuseforestry = NULL,
  savable = TRUE,
  saveable = TRUE
)
}
\arguments{
\item{x}{A data frame of all training predictors.}

\item{y}{A vector of all training responses.}

\item{ntree}{The number of trees to grow in the forest. The default value is
500.}

\item{ntree.first}{The number of trees to grow in the first forest when
trying to determine which features are important.}

\item{ntree.second}{The number of features to use in the second stage when
we grow a second forest using the weights of the first stage.}

\item{replace}{An indicator of whether sampling of training data is with
replacement. The default value is TRUE.}

\item{sampsize}{The size of total samples to draw for the training data. If
sampling with replacement, the default value is the length of the training
data. If samplying without replacement, the default value is two-third of
the length of the training data.}

\item{sample.fraction}{if this is given, then sampsize is ignored and set to
be round(length(y) * sample.fraction). It must be a real number between 0
and 1}

\item{mtry}{The number of variables randomly selected at each split point.
The default value is set to be one third of total number of features of the
training data.}

\item{nodesizeSpl}{Minimum observations contained in terminal nodes. The
default value is 5.}

\item{nodesizeAvg}{Minimum size of terminal nodes for averaging dataset. The
default value is 5.}

\item{nodesizeStrictSpl}{Minimum observations to follow strictly in terminal
nodes. The default value is 1.}

\item{nodesizeStrictAvg}{Minimum size of terminal nodes for averaging data set
to follow strictly when predicting. As of version 0.9.0.8, this parameter
has been reverted to enforce overlap of the averaging data set while
training the forest rather than after training. This means that when using
either version of honesty, splits which leave less than nodesizeStrictAvg
averaging observations in either child node will be rejected, ensuring every
leaf node also has at least nodesizeStrictAvg averaging observations.
The default value is 1.}

\item{minSplitGain}{Minimum loss reduction to split a node further in a tree.}

\item{maxDepth}{Maximum depth of a tree. The default value is 99.}

\item{interactionDepth}{All splits at or above interaction depth must be on variables
that are not weighting variables (as provided by the interactionVariables argument)}

\item{interactionVariables}{Indices of weighting variables.}

\item{featureWeights}{(optional) vector of sampling probablities/weights for
each feature used when subsampling mtry features at each node above or at
interactionDepth. The default is to use uniform probabilities.}

\item{deepFeatureWeights}{used in place of featureWeights for splits below interactionDepth.}

\item{observationWeights}{These denote the weights for each training observation
which determines how likely the observation is to be selected in each bootstrap
sample. This option is not allowed when sampling is done without replacement.}

\item{splitratio}{Proportion of the training data used as the splitting
dataset. It is a ratio between 0 and 1. If the ratio is 1, then essentially
splitting dataset becomes the total entire sampled set and the averaging
dataset is empty. If the ratio is 0, then the splitting data set is empty
and all the data is used for the averaging data set (This is not a good
usage however since there will be no data available for splitting).}

\item{OOBhonest}{This is an experimental method of enforcing honesty. In this
version of honesty, the out-of-bag examples for each tree are used for
another bootstrap draw, which then gives the set of observations in the
honest (averaging) set. This setting also changes how predictions are done
for new examples. When predicting for observations which are out of sample
(using Predict(..., aggregation = "average")), all the trees in the forest are used to predict for the
observation. When predicting for an observation which was in sample (using
predict(..., aggregation = "oob")), only the trees for which the observation was not in the
averaging set (or the set of trees for which the observation was in the splitting set) are
used to make the prediction for the observation. This ensures that the
outcome value for an observation is never used to predict for that observation
even when it is in sample, a feature not shared by the standard honesty
implementation.}

\item{seed}{random seed}

\item{verbose}{Indicator to train the forest in verbose mode}

\item{nthread}{Number of threads to train and predict the forest. The default
number is 0 which represents using all cores.}

\item{splitrule}{only variance is implemented at this point and it contains
specifies the loss function according to which the splits of random forest
should be made}

\item{middleSplit}{if the split value is taking the average of two feature
values. If false, it will take a point based on a uniform distribution
between two feature values. (Default = FALSE)}

\item{maxObs}{The max number of observations to split on}

\item{linear}{Indicator which enables Ridge penalized splits and linear aggregation
functions in the leaf nodes. This is recommended for data with linear outcomes.
For implementation details, see: https://arxiv.org/abs/1906.06463. Default
is FALSE.}

\item{linFeats}{A vector containing the indices of which features to split
linearly on when using linear penalized splits (defaults to use all numerical features).}

\item{monotonicConstraints}{Specifies monotonic relationships between the
continuous features and the outcome. Supplied as a vector of length p with
entries in 1,0,-1 which 1 indicating an increasing monotonic relationship,
-1 indicating a decreasing monotonic relationship, and 0 indicating no
relationship. Constraints supplied for categorical will be ignored.}

\item{monotoneAvg}{This is a boolean flag which indicates whether or not
monotonic constraints should be enforced on the averaging set in addition to the
splitting set. This flag is meaningless unless both honesty and monotonic
constraints are in use. The default is FALSE.}

\item{overfitPenalty}{Value to determine how much to penalize magnitude of
coefficients in ridge regression when using linear splits.}

\item{doubleTree}{if the number of tree is doubled as averaging and splitting
data can be exchanged to create decorrelated trees. (Default = FALSE)}

\item{reuseforestry}{pass in an `forestry` object which will recycle the
dataframe the old object created. It will save some space working on the
same dataset.}

\item{savable}{If TRUE, then RF is created in such a way that it can be
saved and loaded using save(...) and load(...). Setting it to TRUE
(default) will, however, take longer and it will use more memory. When
training many RF, it makes a lot of sense to set this to FALSE to save
time and memory.}

\item{saveable}{deprecated. Do not use.}
}
\value{
Two forestry objects, the first forest, and the adaptive forest,
  as well as the splitting proportions used to grow the second forest.
}
\description{
This is an experimental function where we run forestry in two
  stages, first estimating the feature weights by calculating the relative
  splitting proportions of each feature using a small forest, and then
  growing a much bigger forest using the
  first forest splitting proportions as the featureWeights in the second forest.
}
\details{
adaptiveForestry
}
\examples{

# Set seed for reproductivity
set.seed(292313)

# Use Iris Data
test_idx <- sample(nrow(iris), 11)
x_train <- iris[-test_idx, -1]
y_train <- iris[-test_idx, 1]
x_test <- iris[test_idx, -1]

rf <- adaptiveForestry(x = x_train,
                        y = y_train,
                        ntree.first = 25,
                        ntree.second = 500,
                        nthread = 2)
predict(rf@second.forest, x_test)

}
